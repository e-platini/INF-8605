{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-15T17:25:42.305773500Z",
     "start_time": "2024-09-15T17:25:39.150227700Z"
    }
   },
   "source": [
    "from classication_dataset import ClassificationDataset, get_data_loaders as get_classif_data_loaders\n",
    "from segmentation_dataset import SegmentationDataset, get_data_loaders as get_seg_data_loaders\n",
    "from architectures.simple_CNN import SimpleCNN, train_model as train_classif_model\n",
    "from architectures.unet import Unet, train_model as train_seg_model\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gradcamplpl import gradcamplpl_mask\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "cuda_device = \"cuda:0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T17:25:42.842715800Z",
     "start_time": "2024-09-15T17:25:42.836637300Z"
    }
   },
   "id": "4afed849210d7ba8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A1000 6GB Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader, val_loader, test_loader = get_classif_data_loaders(\n",
    "    img_dir=\"data/PANNUKE/images\",\n",
    "    \n",
    "    label_file=\"data/PANNUKE/labels/labels.npy\",\n",
    "    contiguous_ids_file=\"data/PANNUKE/labels/contiguous_ids.npy\",\n",
    "    \n",
    "    batch_size=64,\n",
    "    val_split=0.2,\n",
    "    test_split=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T17:24:51.857434500Z",
     "start_time": "2024-09-15T17:24:51.849699600Z"
    }
   },
   "id": "de82fb2ad4a5b86d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "model = SimpleCNN()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T17:24:52.105510200Z",
     "start_time": "2024-09-15T17:24:51.994981100Z"
    }
   },
   "id": "a2874870853c63be",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "train_classif_model(model, train_loader, val_loader, cuda_device, learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-13T12:24:31.303228Z",
     "start_time": "2024-09-13T12:22:32.851913Z"
    }
   },
   "id": "e9734b0389e61ef2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 0: 100%|██████████| 23/23 [00:17<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Train loss: 0.7633671941964523\n",
      "Val loss: 0.6926110823949178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 1: 100%|██████████| 23/23 [00:16<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "Train loss: 0.6936085250066675\n",
      "Val loss: 0.6931576225492689\n",
      "No progress on val loss was made, patience reduced to 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 2: 100%|██████████| 23/23 [00:17<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000\n",
      "Train loss: 0.6931565689003986\n",
      "Val loss: 0.6930937621328566\n",
      "No progress on val loss was made, patience reduced to 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 3: 100%|██████████| 23/23 [00:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "Train loss: 0.6932509126870529\n",
      "Val loss: 0.6930651836925082\n",
      "No progress on val loss was made, patience reduced to 0.\n",
      "Patience dropped to 0, changed learning rate to 0.0001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 4: 100%|██████████| 23/23 [00:17<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000\n",
      "Train loss: 0.6931444251019022\n",
      "Val loss: 0.6930723163816664\n",
      "No progress on val loss was made, patience reduced to 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 5:  57%|█████▋    | 13/23 [00:10<00:08,  1.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcuda_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/INF-8605/architectures/simple_CNN.py:95\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, cuda_device, model_save_path, learning_rate)\u001B[0m\n\u001B[1;32m     92\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     93\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining for epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     96\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     97\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:317\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    257\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    171\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    171\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m--> 142\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m    145\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[0;32m~/workspace/venvs/venv310-INF-8605/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:214\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    212\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    213\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[0;32m--> 214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "trained_model_path = \"models/simple_CNN_46\"\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(trained_model_path, weights_only=True))\n",
    "model.eval()\n",
    "model.to(torch.device(cuda_device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T16:37:35.742463100Z",
     "start_time": "2024-09-15T16:37:35.463429400Z"
    }
   },
   "id": "4d93b83817c6cec3",
   "outputs": [
    {
     "data": {
      "text/plain": "SimpleCNN(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=131072, out_features=256, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc2): Linear(in_features=256, out_features=2, bias=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "device = torch.device(cuda_device)\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_inputs_tensors = []\n",
    "\n",
    "    \n",
    "# Disabling gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predicted class (the one with the highest score)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update the total number of samples and correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_predictions.append(predicted.cpu().numpy())\n",
    "        all_inputs_tensors.append(inputs.cpu().numpy())\n",
    "\n",
    "# Flatten the lists to arrays\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_inputs_tensors = np.concatenate(all_inputs_tensors)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Absent', 'Present'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T09:33:17.095619100Z",
     "start_time": "2024-09-15T09:33:15.546924700Z"
    }
   },
   "id": "11d0d01924850ced",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 73.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T16:33:10.219777700Z",
     "start_time": "2024-09-15T16:33:10.216910800Z"
    }
   },
   "id": "7a2c81db772cf63e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=131072, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▏         | 1/64 [00:04<04:25,  4.21s/it]\u001B[A\n",
      "  3%|▎         | 2/64 [00:08<04:31,  4.38s/it]\u001B[A\n",
      "  5%|▍         | 3/64 [00:12<04:24,  4.33s/it]\u001B[A\n",
      "  6%|▋         | 4/64 [00:17<04:24,  4.40s/it]\u001B[A\n",
      "  8%|▊         | 5/64 [00:21<04:21,  4.43s/it]\u001B[A\n",
      "  9%|▉         | 6/64 [00:25<04:07,  4.27s/it]\u001B[A\n",
      " 11%|█         | 7/64 [00:30<04:05,  4.31s/it]\u001B[A\n",
      " 12%|█▎        | 8/64 [00:35<04:10,  4.48s/it]\u001B[A\n",
      " 14%|█▍        | 9/64 [00:39<04:08,  4.51s/it]\u001B[A\n",
      " 16%|█▌        | 10/64 [00:44<04:03,  4.51s/it]\u001B[A\n",
      " 17%|█▋        | 11/64 [00:48<03:54,  4.43s/it]\u001B[A\n",
      " 19%|█▉        | 12/64 [00:53<03:52,  4.47s/it]\u001B[A\n",
      " 20%|██        | 13/64 [00:58<03:59,  4.70s/it]\u001B[A\n",
      " 22%|██▏       | 14/64 [01:09<05:29,  6.58s/it]\u001B[A\n",
      " 23%|██▎       | 15/64 [01:18<06:03,  7.42s/it]\u001B[A\n",
      " 25%|██▌       | 16/64 [01:28<06:33,  8.20s/it]\u001B[A\n",
      " 27%|██▋       | 17/64 [01:37<06:29,  8.29s/it]\u001B[A\n",
      " 28%|██▊       | 18/64 [01:45<06:24,  8.36s/it]\u001B[A\n",
      " 30%|██▉       | 19/64 [01:52<06:02,  8.05s/it]\u001B[A\n",
      " 31%|███▏      | 20/64 [01:57<05:05,  6.94s/it]\u001B[A\n",
      " 33%|███▎      | 21/64 [02:01<04:26,  6.19s/it]\u001B[A\n",
      " 34%|███▍      | 22/64 [02:06<03:58,  5.69s/it]\u001B[A\n",
      " 36%|███▌      | 23/64 [02:10<03:34,  5.23s/it]\u001B[A\n",
      " 38%|███▊      | 24/64 [02:18<04:01,  6.04s/it]\u001B[A\n",
      " 39%|███▉      | 25/64 [02:27<04:26,  6.83s/it]\u001B[A\n",
      " 41%|████      | 26/64 [02:33<04:18,  6.81s/it]\u001B[A\n",
      " 42%|████▏     | 27/64 [02:38<03:43,  6.05s/it]\u001B[A\n",
      " 44%|████▍     | 28/64 [02:42<03:20,  5.57s/it]\u001B[A\n",
      " 45%|████▌     | 29/64 [02:47<03:04,  5.27s/it]\u001B[A\n",
      " 47%|████▋     | 30/64 [02:51<02:47,  4.93s/it]\u001B[A\n",
      " 48%|████▊     | 31/64 [02:55<02:39,  4.84s/it]\u001B[A\n",
      " 50%|█████     | 32/64 [03:00<02:29,  4.67s/it]\u001B[A\n",
      " 52%|█████▏    | 33/64 [03:04<02:25,  4.68s/it]\u001B[A\n",
      " 53%|█████▎    | 34/64 [03:09<02:18,  4.63s/it]\u001B[A\n",
      " 55%|█████▍    | 35/64 [03:13<02:10,  4.51s/it]\u001B[A\n",
      " 56%|█████▋    | 36/64 [03:18<02:07,  4.56s/it]\u001B[A\n",
      " 58%|█████▊    | 37/64 [03:22<02:01,  4.51s/it]\u001B[A\n",
      " 59%|█████▉    | 38/64 [03:26<01:55,  4.46s/it]\u001B[A\n",
      " 61%|██████    | 39/64 [03:31<01:51,  4.48s/it]\u001B[A\n",
      " 62%|██████▎   | 40/64 [03:35<01:47,  4.47s/it]\u001B[A\n",
      " 64%|██████▍   | 41/64 [03:40<01:42,  4.44s/it]\u001B[A\n",
      " 66%|██████▌   | 42/64 [03:44<01:38,  4.46s/it]\u001B[A\n",
      " 67%|██████▋   | 43/64 [03:49<01:33,  4.44s/it]\u001B[A\n",
      " 69%|██████▉   | 44/64 [03:53<01:29,  4.46s/it]\u001B[A\n",
      " 70%|███████   | 45/64 [03:58<01:25,  4.48s/it]\u001B[A\n",
      " 72%|███████▏  | 46/64 [04:03<01:22,  4.58s/it]\u001B[A\n",
      " 73%|███████▎  | 47/64 [04:07<01:16,  4.51s/it]\u001B[A\n",
      " 75%|███████▌  | 48/64 [04:11<01:12,  4.50s/it]\u001B[A\n",
      " 77%|███████▋  | 49/64 [04:16<01:09,  4.61s/it]\u001B[A\n",
      " 78%|███████▊  | 50/64 [04:21<01:03,  4.57s/it]\u001B[A\n",
      " 80%|███████▉  | 51/64 [04:25<00:58,  4.50s/it]\u001B[A\n",
      " 81%|████████▏ | 52/64 [04:29<00:53,  4.47s/it]\u001B[A\n",
      " 83%|████████▎ | 53/64 [04:34<00:48,  4.39s/it]\u001B[A\n",
      " 84%|████████▍ | 54/64 [04:38<00:43,  4.38s/it]\u001B[A\n",
      " 86%|████████▌ | 55/64 [04:42<00:39,  4.41s/it]\u001B[A\n",
      " 88%|████████▊ | 56/64 [04:47<00:36,  4.51s/it]\u001B[A\n",
      " 89%|████████▉ | 57/64 [04:52<00:31,  4.52s/it]\u001B[A\n",
      " 91%|█████████ | 58/64 [04:56<00:27,  4.55s/it]\u001B[A\n",
      " 92%|█████████▏| 59/64 [05:01<00:22,  4.56s/it]\u001B[A\n",
      " 94%|█████████▍| 60/64 [05:05<00:18,  4.53s/it]\u001B[A\n",
      " 95%|█████████▌| 61/64 [05:10<00:13,  4.46s/it]\u001B[A\n",
      " 97%|█████████▋| 62/64 [05:14<00:08,  4.45s/it]\u001B[A\n",
      " 98%|█████████▊| 63/64 [05:19<00:04,  4.45s/it]\u001B[A\n",
      "100%|██████████| 64/64 [05:23<00:00,  5.05s/it]\u001B[A\n",
      " 25%|██▌       | 1/4 [05:23<16:10, 323.59s/it]\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▏         | 1/64 [00:04<04:54,  4.67s/it]\u001B[A\n",
      "  3%|▎         | 2/64 [00:09<04:48,  4.66s/it]\u001B[A\n",
      "  5%|▍         | 3/64 [00:13<04:37,  4.55s/it]\u001B[A\n",
      "  6%|▋         | 4/64 [00:18<04:31,  4.52s/it]\u001B[A\n",
      "  8%|▊         | 5/64 [00:22<04:21,  4.43s/it]\u001B[A\n",
      "  9%|▉         | 6/64 [00:26<04:18,  4.46s/it]\u001B[A\n",
      " 11%|█         | 7/64 [00:31<04:18,  4.54s/it]\u001B[A\n",
      " 12%|█▎        | 8/64 [00:36<04:14,  4.55s/it]\u001B[A\n",
      " 14%|█▍        | 9/64 [00:40<04:08,  4.51s/it]\u001B[A\n",
      " 16%|█▌        | 10/64 [00:44<03:59,  4.43s/it]\u001B[A\n",
      " 17%|█▋        | 11/64 [00:49<03:57,  4.48s/it]\u001B[A\n",
      " 19%|█▉        | 12/64 [00:53<03:50,  4.43s/it]\u001B[A\n",
      " 20%|██        | 13/64 [00:58<03:51,  4.54s/it]\u001B[A\n",
      " 22%|██▏       | 14/64 [01:03<03:46,  4.54s/it]\u001B[A\n",
      " 23%|██▎       | 15/64 [01:07<03:33,  4.36s/it]\u001B[A\n",
      " 25%|██▌       | 16/64 [01:12<03:36,  4.52s/it]\u001B[A\n",
      " 27%|██▋       | 17/64 [01:16<03:32,  4.52s/it]\u001B[A\n",
      " 28%|██▊       | 18/64 [01:23<03:57,  5.17s/it]\u001B[A\n",
      " 30%|██▉       | 19/64 [01:27<03:42,  4.94s/it]\u001B[A\n",
      " 31%|███▏      | 20/64 [01:32<03:32,  4.84s/it]\u001B[A\n",
      " 33%|███▎      | 21/64 [01:40<04:13,  5.90s/it]\u001B[A\n",
      " 34%|███▍      | 22/64 [01:51<05:05,  7.28s/it]\u001B[A\n",
      " 36%|███▌      | 23/64 [01:58<04:57,  7.25s/it]\u001B[A\n",
      " 38%|███▊      | 24/64 [02:03<04:22,  6.57s/it]\u001B[A\n",
      " 39%|███▉      | 25/64 [02:07<03:54,  6.00s/it]\u001B[A\n",
      " 41%|████      | 26/64 [02:12<03:33,  5.62s/it]\u001B[A\n",
      " 42%|████▏     | 27/64 [02:17<03:17,  5.35s/it]\u001B[A\n",
      " 44%|████▍     | 28/64 [02:21<03:04,  5.12s/it]\u001B[A\n",
      " 45%|████▌     | 29/64 [02:26<02:52,  4.93s/it]\u001B[A\n",
      " 47%|████▋     | 30/64 [02:31<02:45,  4.85s/it]\u001B[A\n",
      " 48%|████▊     | 31/64 [02:35<02:38,  4.81s/it]\u001B[A\n",
      " 50%|█████     | 32/64 [02:40<02:29,  4.67s/it]\u001B[A\n",
      " 52%|█████▏    | 33/64 [02:44<02:25,  4.70s/it]\u001B[A\n",
      " 53%|█████▎    | 34/64 [02:49<02:21,  4.72s/it]\u001B[A\n",
      " 55%|█████▍    | 35/64 [02:53<02:10,  4.51s/it]\u001B[A\n",
      " 56%|█████▋    | 36/64 [02:58<02:04,  4.45s/it]\u001B[A\n",
      " 58%|█████▊    | 37/64 [03:02<01:59,  4.41s/it]\u001B[A\n",
      " 59%|█████▉    | 38/64 [03:06<01:54,  4.39s/it]\u001B[A\n",
      " 61%|██████    | 39/64 [03:11<01:49,  4.40s/it]\u001B[A\n",
      " 62%|██████▎   | 40/64 [03:15<01:45,  4.40s/it]\u001B[A\n",
      " 64%|██████▍   | 41/64 [03:20<01:41,  4.42s/it]\u001B[A\n",
      " 66%|██████▌   | 42/64 [03:24<01:39,  4.51s/it]\u001B[A\n",
      " 67%|██████▋   | 43/64 [03:29<01:34,  4.48s/it]\u001B[A\n",
      " 69%|██████▉   | 44/64 [03:33<01:29,  4.48s/it]\u001B[A\n",
      " 70%|███████   | 45/64 [03:38<01:25,  4.48s/it]\u001B[A\n",
      " 72%|███████▏  | 46/64 [03:42<01:19,  4.42s/it]\u001B[A\n",
      " 73%|███████▎  | 47/64 [03:47<01:17,  4.56s/it]\u001B[A\n",
      " 75%|███████▌  | 48/64 [03:51<01:12,  4.54s/it]\u001B[A\n",
      " 77%|███████▋  | 49/64 [03:56<01:07,  4.48s/it]\u001B[A\n",
      " 78%|███████▊  | 50/64 [04:00<01:03,  4.53s/it]\u001B[A\n",
      " 80%|███████▉  | 51/64 [04:05<00:58,  4.47s/it]\u001B[A\n",
      " 81%|████████▏ | 52/64 [04:09<00:53,  4.44s/it]\u001B[A\n",
      " 83%|████████▎ | 53/64 [04:14<00:49,  4.48s/it]\u001B[A\n",
      " 84%|████████▍ | 54/64 [04:18<00:45,  4.59s/it]\u001B[A\n",
      " 86%|████████▌ | 55/64 [04:23<00:41,  4.57s/it]\u001B[A\n",
      " 88%|████████▊ | 56/64 [04:27<00:35,  4.46s/it]\u001B[A\n",
      " 89%|████████▉ | 57/64 [04:32<00:31,  4.45s/it]\u001B[A\n",
      " 91%|█████████ | 58/64 [04:36<00:26,  4.37s/it]\u001B[A\n",
      " 92%|█████████▏| 59/64 [04:40<00:22,  4.47s/it]\u001B[A\n",
      " 94%|█████████▍| 60/64 [04:45<00:17,  4.40s/it]\u001B[A\n",
      " 95%|█████████▌| 61/64 [04:49<00:13,  4.49s/it]\u001B[A\n",
      " 97%|█████████▋| 62/64 [04:54<00:08,  4.44s/it]\u001B[A\n",
      " 98%|█████████▊| 63/64 [04:58<00:04,  4.46s/it]\u001B[A\n",
      "100%|██████████| 64/64 [05:03<00:00,  4.74s/it]\u001B[A\n",
      " 50%|█████     | 2/4 [10:26<10:23, 311.68s/it]\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001B[A\n",
      "  2%|▏         | 1/64 [00:04<04:28,  4.26s/it]\u001B[A\n",
      "  3%|▎         | 2/64 [00:08<04:29,  4.35s/it]\u001B[A\n",
      "  5%|▍         | 3/64 [00:13<04:30,  4.43s/it]\u001B[A\n",
      "  6%|▋         | 4/64 [00:17<04:21,  4.36s/it]\u001B[A\n",
      "  8%|▊         | 5/64 [00:21<04:16,  4.35s/it]\u001B[A\n",
      "  9%|▉         | 6/64 [00:26<04:15,  4.40s/it]\u001B[A\n",
      " 11%|█         | 7/64 [00:30<04:14,  4.46s/it]\u001B[A\n",
      " 12%|█▎        | 8/64 [00:35<04:10,  4.48s/it]\u001B[A\n",
      " 14%|█▍        | 9/64 [00:40<04:14,  4.64s/it]\u001B[A\n",
      " 16%|█▌        | 10/64 [00:45<04:12,  4.68s/it]\u001B[A\n",
      " 17%|█▋        | 11/64 [00:49<04:04,  4.62s/it]\u001B[A\n",
      " 19%|█▉        | 12/64 [00:54<03:57,  4.57s/it]\u001B[A\n",
      " 20%|██        | 13/64 [00:58<03:56,  4.64s/it]\u001B[A\n",
      " 22%|██▏       | 14/64 [01:03<03:48,  4.57s/it]\u001B[A\n",
      " 23%|██▎       | 15/64 [01:07<03:39,  4.47s/it]\u001B[A\n",
      " 25%|██▌       | 16/64 [01:12<03:35,  4.50s/it]\u001B[A\n",
      " 27%|██▋       | 17/64 [01:16<03:31,  4.50s/it]\u001B[A\n",
      " 28%|██▊       | 18/64 [01:20<03:23,  4.43s/it]\u001B[A\n",
      " 30%|██▉       | 19/64 [01:25<03:20,  4.45s/it]\u001B[A\n",
      " 31%|███▏      | 20/64 [01:29<03:14,  4.42s/it]\u001B[A\n",
      " 33%|███▎      | 21/64 [01:33<03:03,  4.27s/it]\u001B[A\n",
      " 34%|███▍      | 22/64 [01:38<03:03,  4.38s/it]\u001B[A\n",
      " 36%|███▌      | 23/64 [01:42<03:00,  4.40s/it]\u001B[A\n",
      " 38%|███▊      | 24/64 [01:46<02:54,  4.36s/it]\u001B[A\n",
      " 39%|███▉      | 25/64 [01:51<02:49,  4.36s/it]\u001B[A\n",
      " 41%|████      | 26/64 [01:56<02:50,  4.50s/it]\u001B[A\n",
      " 42%|████▏     | 27/64 [02:00<02:45,  4.47s/it]\u001B[A\n",
      " 44%|████▍     | 28/64 [02:05<02:42,  4.53s/it]\u001B[A\n",
      " 45%|████▌     | 29/64 [02:09<02:36,  4.47s/it]\u001B[A\n",
      " 47%|████▋     | 30/64 [02:14<02:32,  4.47s/it]\u001B[A\n",
      " 48%|████▊     | 31/64 [02:18<02:31,  4.60s/it]\u001B[A\n",
      " 50%|█████     | 32/64 [02:23<02:27,  4.62s/it]\u001B[A\n",
      " 52%|█████▏    | 33/64 [02:27<02:20,  4.54s/it]\u001B[A\n",
      " 53%|█████▎    | 34/64 [02:32<02:14,  4.48s/it]\u001B[A\n",
      " 55%|█████▍    | 35/64 [02:36<02:09,  4.45s/it]\u001B[A\n",
      " 56%|█████▋    | 36/64 [02:41<02:08,  4.58s/it]\u001B[A\n",
      " 58%|█████▊    | 37/64 [02:46<02:03,  4.57s/it]\u001B[A\n",
      " 59%|█████▉    | 38/64 [02:50<01:58,  4.55s/it]\u001B[A\n",
      " 61%|██████    | 39/64 [02:55<01:52,  4.51s/it]\u001B[A\n",
      " 62%|██████▎   | 40/64 [02:59<01:51,  4.63s/it]\u001B[A\n",
      " 64%|██████▍   | 41/64 [03:04<01:46,  4.64s/it]\u001B[A\n",
      " 66%|██████▌   | 42/64 [03:09<01:42,  4.66s/it]\u001B[A\n",
      " 67%|██████▋   | 43/64 [03:13<01:37,  4.64s/it]\u001B[A\n",
      " 69%|██████▉   | 44/64 [03:18<01:31,  4.56s/it]\u001B[A\n",
      " 70%|███████   | 45/64 [03:22<01:26,  4.55s/it]\u001B[A\n",
      " 72%|███████▏  | 46/64 [03:27<01:21,  4.51s/it]\u001B[A\n",
      " 73%|███████▎  | 47/64 [03:31<01:15,  4.42s/it]\u001B[A\n",
      " 75%|███████▌  | 48/64 [03:35<01:10,  4.40s/it]\u001B[A\n",
      " 77%|███████▋  | 49/64 [03:40<01:05,  4.39s/it]\u001B[A\n",
      " 78%|███████▊  | 50/64 [03:44<01:01,  4.42s/it]\u001B[A\n",
      " 80%|███████▉  | 51/64 [03:48<00:57,  4.39s/it]\u001B[A\n",
      " 81%|████████▏ | 52/64 [03:53<00:52,  4.39s/it]\u001B[A\n",
      " 83%|████████▎ | 53/64 [03:57<00:48,  4.37s/it]\u001B[A\n",
      " 84%|████████▍ | 54/64 [04:02<00:43,  4.37s/it]\u001B[A\n",
      " 86%|████████▌ | 55/64 [04:06<00:39,  4.38s/it]\u001B[A\n",
      " 88%|████████▊ | 56/64 [04:10<00:35,  4.41s/it]\u001B[A\n",
      " 89%|████████▉ | 57/64 [04:15<00:31,  4.48s/it]\u001B[A\n",
      " 91%|█████████ | 58/64 [04:20<00:26,  4.49s/it]\u001B[A\n",
      " 92%|█████████▏| 59/64 [04:24<00:22,  4.49s/it]\u001B[A\n",
      " 94%|█████████▍| 60/64 [04:28<00:17,  4.46s/it]\u001B[A\n",
      " 95%|█████████▌| 61/64 [04:33<00:13,  4.42s/it]\u001B[A\n",
      " 97%|█████████▋| 62/64 [04:37<00:08,  4.44s/it]\u001B[A\n",
      " 98%|█████████▊| 63/64 [04:41<00:04,  4.38s/it]\u001B[A\n",
      "100%|██████████| 64/64 [04:46<00:00,  4.48s/it]\u001B[A\n",
      " 75%|███████▌  | 3/4 [15:13<05:00, 300.31s/it]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      " 12%|█▎        | 1/8 [00:04<00:30,  4.35s/it]\u001B[A\n",
      " 25%|██▌       | 2/8 [00:09<00:27,  4.59s/it]\u001B[A\n",
      " 38%|███▊      | 3/8 [00:13<00:22,  4.51s/it]\u001B[A\n",
      " 50%|█████     | 4/8 [00:17<00:17,  4.49s/it]\u001B[A\n",
      " 62%|██████▎   | 5/8 [00:22<00:13,  4.56s/it]\u001B[A\n",
      " 75%|███████▌  | 6/8 [00:26<00:08,  4.43s/it]\u001B[A\n",
      " 88%|████████▊ | 7/8 [00:31<00:04,  4.49s/it]\u001B[A\n",
      "100%|██████████| 8/8 [00:36<00:00,  4.52s/it]\u001B[A\n",
      "100%|██████████| 4/4 [15:49<00:00, 237.47s/it]\n"
     ]
    }
   ],
   "source": [
    "target_layer = model.conv6\n",
    "grad_maps = []\n",
    "\n",
    "for _, _, ids in tqdm(test_loader):\n",
    "    for _id in tqdm(ids.tolist()):\n",
    "        img_path = f\"data/PANNUKE/images/{_id}.png\"\n",
    "        grad_maps.append(gradcamplpl_mask(model, target_layer, img_path, cuda_device))\n",
    "    \n",
    "np.save(\"CAM_outputs/maps.npy\",grad_maps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T16:59:44.789187200Z",
     "start_time": "2024-09-15T16:43:54.908397100Z"
    }
   },
   "id": "2b6518af5707564e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "seg_model = Unet()\n",
    "seg_train_loader, seg_val_loader, seg_test_loader = get_seg_data_loaders(\n",
    "    img_dir=\"data/PANNUKE/images\",\n",
    "    mask_dir=\"data/PANNUKE/masks\",\n",
    "    contiguous_ids_file=\"data/PANNUKE/labels/contiguous_ids.npy\",\n",
    "    batch_size=64,\n",
    "    val_split=0.2,\n",
    "    test_split=0.1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T17:25:49.730456700Z",
     "start_time": "2024-09-15T17:25:49.626010800Z"
    }
   },
   "id": "d9b67926a72210ba",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training for epoch 0:   0%|          | 0/23 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 19.60 GiB is allocated by PyTorch, and 780.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_seg_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseg_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseg_train_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseg_val_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcuda_device\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Drive\\work\\classes\\INF8605\\INF-8605\\architectures\\unet.py:116\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, cuda_device, model_save_path, learning_rate)\u001B[0m\n\u001B[0;32m    113\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    115\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Zero the parameter gradients\u001B[39;00m\n\u001B[1;32m--> 116\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m    117\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)  \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[0;32m    118\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Drive\\work\\classes\\INF8605\\INF-8605\\architectures\\unet.py:77\u001B[0m, in \u001B[0;36mUnet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     74\u001B[0m         x \u001B[38;5;241m=\u001B[39m TF\u001B[38;5;241m.\u001B[39mresize(x, size\u001B[38;5;241m=\u001B[39mskip_connection\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:])\n\u001B[0;32m     76\u001B[0m     concat_skip \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((skip_connection, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 77\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mups\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconcat_skip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_conv(x)\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Drive\\work\\classes\\INF8605\\INF-8605\\architectures\\unet.py:23\u001B[0m, in \u001B[0;36mDoubleConv.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\venvs\\venv311-INF8605\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 19.60 GiB is allocated by PyTorch, and 780.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_seg_model(seg_model, seg_train_loader, seg_val_loader, cuda_device, learning_rate=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-15T17:26:13.465619600Z",
     "start_time": "2024-09-15T17:25:49.913310300Z"
    }
   },
   "id": "9e195afaa52b458e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc7769211585186"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
